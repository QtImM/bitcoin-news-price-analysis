{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置随机种子\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 数据加载与预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path):\n",
    "    # 加载数据\n",
    "    df = pd.read_csv(file_path, parse_dates=['date'])\n",
    "    print(\"数据加载完成，开始特征工程...\")\n",
    "    \n",
    "    # 基础特征工程\n",
    "    # 价格变化特征\n",
    "    df['returns'] = df['price'].pct_change()\n",
    "    df['returns_2'] = df['price'].pct_change(2)\n",
    "    df['returns_5'] = df['price'].pct_change(5)\n",
    "    \n",
    "    # 波动率特征\n",
    "    for window in [3, 5, 10, 20]:\n",
    "        df[f'volatility_{window}d'] = df['returns'].rolling(window).std()\n",
    "    \n",
    "    # 移动平均特征\n",
    "    for window in [5, 10, 20]:\n",
    "        df[f'ma_{window}'] = df['price'].rolling(window).mean()\n",
    "        df[f'ma_ratio_{window}'] = df['price'] / df[f'ma_{window}']\n",
    "    \n",
    "    # RSI指标\n",
    "    def calculate_rsi(prices, period=14):\n",
    "        delta = prices.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "        rs = gain / loss\n",
    "        return 100 - (100 / (1 + rs))\n",
    "    \n",
    "    df['rsi_14'] = calculate_rsi(df['price'])\n",
    "    \n",
    "    # MACD指标\n",
    "    exp1 = df['price'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df['price'].ewm(span=26, adjust=False).mean()\n",
    "    df['macd'] = exp1 - exp2\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()\n",
    "    df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "    \n",
    "    # 情感特征\n",
    "    df['sentiment_ma_3'] = df['sentiment_score'].rolling(3).mean()\n",
    "    df['sentiment_ma_7'] = df['sentiment_score'].rolling(7).mean()\n",
    "    \n",
    "    # 目标变量：未来3天的价格变化率\n",
    "    df['target'] = df['price'].pct_change(3).shift(-3)\n",
    "    \n",
    "    # 删除缺失值\n",
    "    df = df.dropna()\n",
    "    \n",
    "    print(\"特征工程完成，开始特征选择...\")\n",
    "    \n",
    "    # 特征选择\n",
    "    features = [\n",
    "        'returns', 'returns_2', 'returns_5',\n",
    "        'volatility_3d', 'volatility_5d', 'volatility_10d', 'volatility_20d',\n",
    "        'ma_ratio_5', 'ma_ratio_10', 'ma_ratio_20',\n",
    "        'rsi_14', 'macd', 'macd_hist',\n",
    "        'sentiment_score', 'sentiment_ma_3', 'sentiment_ma_7'\n",
    "    ]\n",
    "    \n",
    "    return df, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.创建特征窗口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共创建 20002 个事件窗口\n",
      "每个窗口形状: (9, 16)\n",
      "特征维度: (20002, 9, 16)\n",
      "目标维度: (20002,)\n"
     ]
    }
   ],
   "source": [
    "# 定义参数\n",
    "window_size = (-3, 5)  # 事件窗口范围\n",
    "windows, targets = [], []\n",
    "\n",
    "\n",
    "# 找到所有有新闻的日期（假设sentiment_score非零表示有新闻）\n",
    "event_dates = df[df['sentiment_score'].abs() > 0.01].index\n",
    "\n",
    "for idx in event_dates:\n",
    "    start = idx + window_size[0]\n",
    "    end = idx + window_size[1] + 1\n",
    "    \n",
    "    if start >= 0 and end <= len(df):\n",
    "        try:\n",
    "            # 提取特征并确保转换为numpy数组\n",
    "            window_data = df.iloc[start:end][features].values\n",
    "            \n",
    "            # 添加时间编码\n",
    "            time_pos = np.linspace(0, 1, end-start).reshape(-1, 1)\n",
    "            window_data = np.hstack([window_data, time_pos])\n",
    "            \n",
    "            # 计算目标（未来3天平均收益率）\n",
    "            if end + 3 <= len(df):\n",
    "                future_window = df.iloc[end:end+3]\n",
    "                target = future_window['price'].mean() / df.iloc[end-1]['price'] - 1\n",
    "                \n",
    "                # 确保数据类型正确\n",
    "                window_data = np.array(window_data, dtype=np.float32)\n",
    "                target = np.float32(target)\n",
    "                \n",
    "                windows.append(window_data)\n",
    "                targets.append(target)\n",
    "        except Exception as e:\n",
    "            print(f\"处理索引 {idx} 时出错: {e}\")\n",
    "            continue\n",
    "\n",
    "# 转换为numpy数组\n",
    "windows = np.array(windows)\n",
    "targets = np.array(targets)\n",
    "\n",
    "print(f\"共创建 {len(windows)} 个事件窗口\")\n",
    "if len(windows) > 0:\n",
    "    print(f\"每个窗口形状: {windows[0].shape}\")\n",
    "    print(f\"特征维度: {windows.shape}\")\n",
    "    print(f\"目标维度: {targets.shape}\")\n",
    "else:\n",
    "    print(\"警告：未创建任何有效的事件窗口\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.创建PyTorch数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: inputs shape: torch.Size([64, 15]), targets shape: torch.Size([64])\n",
      "\n",
      "训练样本: 24812 | 验证样本: 6204\n",
      "\n",
      "目标值统计:\n",
      "训练集: Min=-3.3910, Max=2.5637, Mean=0.0003, Std=1.0024\n",
      "验证集: Min=-3.3910, Max=2.5637, Mean=-0.0010, Std=0.9903\n"
     ]
    }
   ],
   "source": [
    "class CryptoDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.targets = torch.FloatTensor(targets)  # 使用FloatTensor而不是LongTensor\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "# 数据预处理\n",
    "# 方案：直接使用百分比变化作为目标变量，不做对数变换\n",
    "df['target'] = df['change_percent']\n",
    "\n",
    "# 处理极端值（可选）\n",
    "# 使用百分位数截断法处理异常值\n",
    "q_low = df['target'].quantile(0.01)\n",
    "q_high = df['target'].quantile(0.99)\n",
    "df['target'] = df['target'].clip(q_low, q_high)\n",
    "\n",
    "# 选择特征和目标\n",
    "features = [\n",
    "    'returns', 'returns_2', 'returns_5', \n",
    "    'vol_3', 'vol_5', 'vol_10', 'vol_20', \n",
    "    'ma_5', 'ma_20', 'ma_ratio', 'price_ma5_ratio',\n",
    "    'rsi_14', 'macd', 'macd_hist', 'sentiment_score'\n",
    "]\n",
    "X = df[features].values  # 确保转换为numpy数组\n",
    "y = df['target'].values\n",
    "\n",
    "# 对特征进行标准化处理\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# 标准化特征和目标变量\n",
    "X_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "X = X_scaler.fit_transform(X)\n",
    "y = y_scaler.fit_transform(y.reshape(-1, 1)).ravel()\n",
    "\n",
    "# 划分训练集/测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    shuffle=True,  # 确保数据被打乱\n",
    ")\n",
    "\n",
    "# 创建数据集实例\n",
    "train_dataset = CryptoDataset(X_train, y_train)\n",
    "val_dataset = CryptoDataset(X_test, y_test)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=True,\n",
    "    drop_last=True  # 丢弃最后一个不完整的批次\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=64,\n",
    "    drop_last=True\n",
    ")\n",
    "# 验证数据加载是否正确\n",
    "for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx}: inputs shape: {inputs.shape}, targets shape: {targets.shape}\")\n",
    "    if batch_idx == 0:  # 只打印第一个批次\n",
    "        break\n",
    "\n",
    "# 打印数据集信息\n",
    "print(f\"\\n训练样本: {len(X_train)} | 验证样本: {len(X_test)}\")\n",
    "print(\"\\n目标值统计:\")\n",
    "print(f\"训练集: Min={np.min(y_train):.4f}, Max={np.max(y_train):.4f}, Mean={np.mean(y_train):.4f}, Std={np.std(y_train):.4f}\")\n",
    "print(f\"验证集: Min={np.min(y_test):.4f}, Max={np.max(y_test):.4f}, Mean={np.mean(y_test):.4f}, Std={np.std(y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -i https://mirrors.aliyun.com/pypi/simple/ transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImprovedBiLSTMAttention(\n",
      "  (feature_projection): Sequential(\n",
      "    (0): Linear(in_features=15, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (lstm): LSTM(128, 128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (attention): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      "  (regressor): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (7): Dropout(p=0.3, inplace=False)\n",
      "    (8): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "class ImprovedBiLSTMAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.3):\n",
    "        super(ImprovedBiLSTMAttention, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # 特征投影层\n",
    "        self.feature_projection = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # 双向LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            hidden_dim,\n",
    "            hidden_dim,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # 自注意力机制\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # 回归输出层\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "        # 初始化权重\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.LSTM):\n",
    "            for name, param in module.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    nn.init.orthogonal_(param)\n",
    "                elif 'bias' in name:\n",
    "                    nn.init.zeros_(param)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 特征投影\n",
    "        x = self.feature_projection(x)\n",
    "        x = x.unsqueeze(1)  # 添加序列维度\n",
    "        \n",
    "        # LSTM处理\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # 注意力计算\n",
    "        attention_weights = self.attention(lstm_out)\n",
    "        attention_weights = F.softmax(attention_weights, dim=1)\n",
    "        \n",
    "        # 计算上下文向量\n",
    "        context = torch.bmm(attention_weights.transpose(1, 2), lstm_out)\n",
    "        context = context.squeeze(1)\n",
    "        \n",
    "        # 回归预测\n",
    "        output = self.regressor(context)\n",
    "        return output.squeeze(-1)\n",
    "\n",
    "# 创建模型实例\n",
    "model = ImprovedBiLSTMAttention(\n",
    "    input_dim=len(features),\n",
    "    hidden_dim=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.SmoothL1Loss()  # 使用Huber损失，对异常值更稳定\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0001,  # 降低学习率\n",
    "    weight_decay=0.01,  # 增加L2正则化\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    verbose=True,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# 打印模型结构\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.配置训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义训练参数\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "early_stopping_patience = 10  # 添加早停参数\n",
    "best_val_loss = float('inf')\n",
    "history = {'train': [], 'val': []}\n",
    "no_improve_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN in outputs at batch 0\n",
      "Warning: NaN in outputs at batch 1\n",
      "Warning: NaN in outputs at batch 2\n",
      "Warning: NaN in outputs at batch 3\n",
      "Warning: NaN in outputs at batch 4\n",
      "Warning: NaN in outputs at batch 5\n",
      "Warning: NaN in outputs at batch 6\n",
      "Warning: NaN in outputs at batch 7\n",
      "Warning: NaN in outputs at batch 8\n",
      "Warning: NaN in outputs at batch 9\n",
      "Warning: NaN in outputs at batch 10\n",
      "Warning: NaN in outputs at batch 11\n",
      "Warning: NaN in outputs at batch 12\n",
      "Warning: NaN in outputs at batch 13\n",
      "Warning: NaN in outputs at batch 14\n",
      "Warning: NaN in outputs at batch 15\n",
      "Warning: NaN in outputs at batch 16\n",
      "Warning: NaN in outputs at batch 17\n",
      "Warning: NaN in outputs at batch 18\n",
      "Warning: NaN in outputs at batch 19\n",
      "Warning: NaN in outputs at batch 20\n",
      "Warning: NaN in outputs at batch 21\n",
      "Warning: NaN in outputs at batch 22\n",
      "Warning: NaN in outputs at batch 23\n",
      "Warning: NaN in outputs at batch 24\n",
      "Warning: NaN in outputs at batch 25\n",
      "Warning: NaN in outputs at batch 26\n",
      "Warning: NaN in outputs at batch 27\n",
      "Warning: NaN in outputs at batch 28\n",
      "Warning: NaN in outputs at batch 29\n",
      "Warning: NaN in outputs at batch 30\n",
      "Warning: NaN in outputs at batch 31\n",
      "Warning: NaN in outputs at batch 32\n",
      "Warning: NaN in outputs at batch 33\n",
      "Warning: NaN in outputs at batch 34\n",
      "Warning: NaN in outputs at batch 35\n",
      "Warning: NaN in outputs at batch 36\n",
      "Warning: NaN in outputs at batch 37\n",
      "Warning: NaN in outputs at batch 38\n",
      "Warning: NaN in outputs at batch 39\n",
      "Warning: NaN in outputs at batch 40\n",
      "Warning: NaN in outputs at batch 41\n",
      "Warning: NaN in outputs at batch 42\n",
      "Warning: NaN in outputs at batch 43\n",
      "Warning: NaN in outputs at batch 44\n",
      "Warning: NaN in outputs at batch 45\n",
      "Warning: NaN in outputs at batch 46\n",
      "Warning: NaN in outputs at batch 47\n",
      "Warning: NaN in outputs at batch 48\n",
      "Warning: NaN in outputs at batch 49\n",
      "Warning: NaN in outputs at batch 50\n",
      "Warning: NaN in outputs at batch 51\n",
      "Warning: NaN in outputs at batch 52\n",
      "Warning: NaN in outputs at batch 53\n",
      "Warning: NaN in outputs at batch 54\n",
      "Warning: NaN in outputs at batch 55\n",
      "Warning: NaN in outputs at batch 56\n",
      "Warning: NaN in outputs at batch 57\n",
      "Warning: NaN in outputs at batch 58\n",
      "Warning: NaN in outputs at batch 59\n",
      "Warning: NaN in outputs at batch 60\n",
      "Warning: NaN in outputs at batch 61\n",
      "Warning: NaN in outputs at batch 62\n",
      "Warning: NaN in outputs at batch 63\n",
      "Warning: NaN in outputs at batch 64\n",
      "Warning: NaN in outputs at batch 65\n",
      "Warning: NaN in outputs at batch 66\n",
      "Warning: NaN in outputs at batch 67\n",
      "Warning: NaN in outputs at batch 68\n",
      "Warning: NaN in outputs at batch 69\n",
      "Warning: NaN in outputs at batch 70\n",
      "Warning: NaN in outputs at batch 71\n",
      "Warning: NaN in outputs at batch 72\n",
      "Warning: NaN in outputs at batch 73\n",
      "Warning: NaN in outputs at batch 74\n",
      "Warning: NaN in outputs at batch 75\n",
      "Warning: NaN in outputs at batch 76\n",
      "Warning: NaN in outputs at batch 77\n",
      "Warning: NaN in outputs at batch 78\n",
      "Warning: NaN in outputs at batch 79\n",
      "Warning: NaN in outputs at batch 80\n",
      "Warning: NaN in outputs at batch 81\n",
      "Warning: NaN in outputs at batch 82\n",
      "Warning: NaN in outputs at batch 83\n",
      "Warning: NaN in outputs at batch 84\n",
      "Warning: NaN in outputs at batch 85\n",
      "Warning: NaN in outputs at batch 86\n",
      "Warning: NaN in outputs at batch 87\n",
      "Warning: NaN in outputs at batch 88\n",
      "Warning: NaN in outputs at batch 89\n",
      "Warning: NaN in outputs at batch 90\n",
      "Warning: NaN in outputs at batch 91\n",
      "Warning: NaN in outputs at batch 92\n",
      "Warning: NaN in outputs at batch 93\n",
      "Warning: NaN in outputs at batch 94\n",
      "Warning: NaN in outputs at batch 95\n",
      "Warning: NaN in outputs at batch 96\n",
      "Warning: NaN in outputs at batch 97\n",
      "Warning: NaN in outputs at batch 98\n",
      "Warning: NaN in outputs at batch 99\n",
      "Warning: NaN in outputs at batch 100\n",
      "Warning: NaN in outputs at batch 101\n",
      "Warning: NaN in outputs at batch 102\n",
      "Warning: NaN in outputs at batch 103\n",
      "Warning: NaN in outputs at batch 104\n",
      "Warning: NaN in outputs at batch 105\n",
      "Warning: NaN in outputs at batch 106\n",
      "Warning: NaN in outputs at batch 107\n",
      "Warning: NaN in outputs at batch 108\n",
      "Warning: NaN in outputs at batch 109\n",
      "Warning: NaN in outputs at batch 110\n",
      "Warning: NaN in outputs at batch 111\n",
      "Warning: NaN in outputs at batch 112\n",
      "Warning: NaN in outputs at batch 113\n",
      "Warning: NaN in outputs at batch 114\n",
      "Warning: NaN in outputs at batch 115\n",
      "Warning: NaN in outputs at batch 116\n",
      "Warning: NaN in outputs at batch 117\n",
      "Warning: NaN in outputs at batch 118\n",
      "Warning: NaN in outputs at batch 119\n",
      "Warning: NaN in outputs at batch 120\n",
      "Warning: NaN in outputs at batch 121\n",
      "Warning: NaN in outputs at batch 122\n",
      "Warning: NaN in outputs at batch 123\n",
      "Warning: NaN in outputs at batch 124\n",
      "Warning: NaN in outputs at batch 125\n",
      "Warning: NaN in outputs at batch 126\n",
      "Warning: NaN in outputs at batch 127\n",
      "Warning: NaN in outputs at batch 128\n",
      "Warning: NaN in outputs at batch 129\n",
      "Warning: NaN in outputs at batch 130\n",
      "Warning: NaN in outputs at batch 131\n",
      "Warning: NaN in outputs at batch 132\n",
      "Warning: NaN in outputs at batch 133\n",
      "Warning: NaN in outputs at batch 134\n",
      "Warning: NaN in outputs at batch 135\n",
      "Warning: NaN in outputs at batch 136\n",
      "Warning: NaN in outputs at batch 137\n",
      "Warning: NaN in outputs at batch 138\n",
      "Warning: NaN in outputs at batch 139\n",
      "Warning: NaN in outputs at batch 140\n",
      "Warning: NaN in outputs at batch 141\n",
      "Warning: NaN in outputs at batch 142\n",
      "Warning: NaN in outputs at batch 143\n",
      "Warning: NaN in outputs at batch 144\n",
      "Warning: NaN in outputs at batch 145\n",
      "Warning: NaN in outputs at batch 146\n",
      "Warning: NaN in outputs at batch 147\n",
      "Warning: NaN in outputs at batch 148\n",
      "Warning: NaN in outputs at batch 149\n",
      "Warning: NaN in outputs at batch 150\n",
      "Warning: NaN in outputs at batch 151\n",
      "Warning: NaN in outputs at batch 152\n",
      "Warning: NaN in outputs at batch 153\n",
      "Warning: NaN in outputs at batch 154\n",
      "Warning: NaN in outputs at batch 155\n",
      "Warning: NaN in outputs at batch 156\n",
      "Warning: NaN in outputs at batch 157\n",
      "Warning: NaN in outputs at batch 158\n",
      "Warning: NaN in outputs at batch 159\n",
      "Warning: NaN in outputs at batch 160\n",
      "Warning: NaN in outputs at batch 161\n",
      "Warning: NaN in outputs at batch 162\n",
      "Warning: NaN in outputs at batch 163\n",
      "Warning: NaN in outputs at batch 164\n",
      "Warning: NaN in outputs at batch 165\n",
      "Warning: NaN in outputs at batch 166\n",
      "Warning: NaN in outputs at batch 167\n",
      "Warning: NaN in outputs at batch 168\n",
      "Warning: NaN in outputs at batch 169\n",
      "Warning: NaN in outputs at batch 170\n",
      "Warning: NaN in outputs at batch 171\n",
      "Warning: NaN in outputs at batch 172\n",
      "Warning: NaN in outputs at batch 173\n",
      "Warning: NaN in outputs at batch 174\n",
      "Warning: NaN in outputs at batch 175\n",
      "Warning: NaN in outputs at batch 176\n",
      "Warning: NaN in outputs at batch 177\n",
      "Warning: NaN in outputs at batch 178\n",
      "Warning: NaN in outputs at batch 179\n",
      "Warning: NaN in outputs at batch 180\n",
      "Warning: NaN in outputs at batch 181\n",
      "Warning: NaN in outputs at batch 182\n",
      "Warning: NaN in outputs at batch 183\n",
      "Warning: NaN in outputs at batch 184\n",
      "Warning: NaN in outputs at batch 185\n",
      "Warning: NaN in outputs at batch 186\n",
      "Warning: NaN in outputs at batch 187\n",
      "Warning: NaN in outputs at batch 188\n",
      "Warning: NaN in outputs at batch 189\n",
      "Warning: NaN in outputs at batch 190\n",
      "Warning: NaN in outputs at batch 191\n",
      "Warning: NaN in outputs at batch 192\n",
      "Warning: NaN in outputs at batch 193\n",
      "Warning: NaN in outputs at batch 194\n",
      "Warning: NaN in outputs at batch 195\n",
      "Warning: NaN in outputs at batch 196\n",
      "Warning: NaN in outputs at batch 197\n",
      "Warning: NaN in outputs at batch 198\n",
      "Warning: NaN in outputs at batch 199\n",
      "Warning: NaN in outputs at batch 200\n",
      "Warning: NaN in outputs at batch 201\n",
      "Warning: NaN in outputs at batch 202\n",
      "Warning: NaN in outputs at batch 203\n",
      "Warning: NaN in outputs at batch 204\n",
      "Warning: NaN in outputs at batch 205\n",
      "Warning: NaN in outputs at batch 206\n",
      "Warning: NaN in outputs at batch 207\n",
      "Warning: NaN in outputs at batch 208\n",
      "Warning: NaN in outputs at batch 209\n",
      "Warning: NaN in outputs at batch 210\n",
      "Warning: NaN in outputs at batch 211\n",
      "Warning: NaN in outputs at batch 212\n",
      "Warning: NaN in outputs at batch 213\n",
      "Warning: NaN in outputs at batch 214\n",
      "Warning: NaN in outputs at batch 215\n",
      "Warning: NaN in outputs at batch 216\n",
      "Warning: NaN in outputs at batch 217\n",
      "Warning: NaN in outputs at batch 218\n",
      "Warning: NaN in outputs at batch 219\n",
      "Warning: NaN in outputs at batch 220\n",
      "Warning: NaN in outputs at batch 221\n",
      "Warning: NaN in outputs at batch 222\n",
      "Warning: NaN in outputs at batch 223\n",
      "Warning: NaN in outputs at batch 224\n",
      "Warning: NaN in outputs at batch 225\n",
      "Warning: NaN in outputs at batch 226\n",
      "Warning: NaN in outputs at batch 227\n",
      "Warning: NaN in outputs at batch 228\n",
      "Warning: NaN in outputs at batch 229\n",
      "Warning: NaN in outputs at batch 230\n",
      "Warning: NaN in outputs at batch 231\n",
      "Warning: NaN in outputs at batch 232\n",
      "Warning: NaN in outputs at batch 233\n",
      "Warning: NaN in outputs at batch 234\n",
      "Warning: NaN in outputs at batch 235\n",
      "Warning: NaN in outputs at batch 236\n",
      "Warning: NaN in outputs at batch 237\n",
      "Warning: NaN in outputs at batch 238\n",
      "Warning: NaN in outputs at batch 239\n",
      "Warning: NaN in outputs at batch 240\n",
      "Warning: NaN in outputs at batch 241\n",
      "Warning: NaN in outputs at batch 242\n",
      "Warning: NaN in outputs at batch 243\n",
      "Warning: NaN in outputs at batch 244\n",
      "Warning: NaN in outputs at batch 245\n",
      "Warning: NaN in outputs at batch 246\n",
      "Warning: NaN in outputs at batch 247\n",
      "Warning: NaN in outputs at batch 248\n",
      "Warning: NaN in outputs at batch 249\n",
      "Warning: NaN in outputs at batch 250\n",
      "Warning: NaN in outputs at batch 251\n",
      "Warning: NaN in outputs at batch 252\n",
      "Warning: NaN in outputs at batch 253\n",
      "Warning: NaN in outputs at batch 254\n",
      "Warning: NaN in outputs at batch 255\n",
      "Warning: NaN in outputs at batch 256\n",
      "Warning: NaN in outputs at batch 257\n",
      "Warning: NaN in outputs at batch 258\n",
      "Warning: NaN in outputs at batch 259\n",
      "Warning: NaN in outputs at batch 260\n",
      "Warning: NaN in outputs at batch 261\n",
      "Warning: NaN in outputs at batch 262\n",
      "Warning: NaN in outputs at batch 263\n",
      "Warning: NaN in outputs at batch 264\n",
      "Warning: NaN in outputs at batch 265\n",
      "Warning: NaN in outputs at batch 266\n",
      "Warning: NaN in outputs at batch 267\n",
      "Warning: NaN in outputs at batch 268\n",
      "Warning: NaN in outputs at batch 269\n",
      "Warning: NaN in outputs at batch 270\n",
      "Warning: NaN in outputs at batch 271\n",
      "Warning: NaN in outputs at batch 272\n",
      "Warning: NaN in outputs at batch 273\n",
      "Warning: NaN in outputs at batch 274\n",
      "Warning: NaN in outputs at batch 275\n",
      "Warning: NaN in outputs at batch 276\n",
      "Warning: NaN in outputs at batch 277\n",
      "Warning: NaN in outputs at batch 278\n",
      "Warning: NaN in outputs at batch 279\n",
      "Warning: NaN in outputs at batch 280\n",
      "Warning: NaN in outputs at batch 281\n",
      "Warning: NaN in outputs at batch 282\n",
      "Warning: NaN in outputs at batch 283\n",
      "Warning: NaN in outputs at batch 284\n",
      "Warning: NaN in outputs at batch 285\n",
      "Warning: NaN in outputs at batch 286\n",
      "Warning: NaN in outputs at batch 287\n",
      "Warning: NaN in outputs at batch 288\n",
      "Warning: NaN in outputs at batch 289\n",
      "Warning: NaN in outputs at batch 290\n",
      "Warning: NaN in outputs at batch 291\n",
      "Warning: NaN in outputs at batch 292\n",
      "Warning: NaN in outputs at batch 293\n",
      "Warning: NaN in outputs at batch 294\n",
      "Warning: NaN in outputs at batch 295\n",
      "Warning: NaN in outputs at batch 296\n",
      "Warning: NaN in outputs at batch 297\n",
      "Warning: NaN in outputs at batch 298\n",
      "Warning: NaN in outputs at batch 299\n",
      "Warning: NaN in outputs at batch 300\n",
      "Warning: NaN in outputs at batch 301\n",
      "Warning: NaN in outputs at batch 302\n",
      "Warning: NaN in outputs at batch 303\n",
      "Warning: NaN in outputs at batch 304\n",
      "Warning: NaN in outputs at batch 305\n",
      "Warning: NaN in outputs at batch 306\n",
      "Warning: NaN in outputs at batch 307\n",
      "Warning: NaN in outputs at batch 308\n",
      "Warning: NaN in outputs at batch 309\n",
      "Warning: NaN in outputs at batch 310\n",
      "Warning: NaN in outputs at batch 311\n",
      "Warning: NaN in outputs at batch 312\n",
      "Warning: NaN in outputs at batch 313\n",
      "Warning: NaN in outputs at batch 314\n",
      "Warning: NaN in outputs at batch 315\n",
      "Warning: NaN in outputs at batch 316\n",
      "Warning: NaN in outputs at batch 317\n",
      "Warning: NaN in outputs at batch 318\n",
      "Warning: NaN in outputs at batch 319\n",
      "Warning: NaN in outputs at batch 320\n",
      "Warning: NaN in outputs at batch 321\n",
      "Warning: NaN in outputs at batch 322\n",
      "Warning: NaN in outputs at batch 323\n",
      "Warning: NaN in outputs at batch 324\n",
      "Warning: NaN in outputs at batch 325\n",
      "Warning: NaN in outputs at batch 326\n",
      "Warning: NaN in outputs at batch 327\n",
      "Warning: NaN in outputs at batch 328\n",
      "Warning: NaN in outputs at batch 329\n",
      "Warning: NaN in outputs at batch 330\n",
      "Warning: NaN in outputs at batch 331\n",
      "Warning: NaN in outputs at batch 332\n",
      "Warning: NaN in outputs at batch 333\n",
      "Warning: NaN in outputs at batch 334\n",
      "Warning: NaN in outputs at batch 335\n",
      "Warning: NaN in outputs at batch 336\n",
      "Warning: NaN in outputs at batch 337\n",
      "Warning: NaN in outputs at batch 338\n",
      "Warning: NaN in outputs at batch 339\n",
      "Warning: NaN in outputs at batch 340\n",
      "Warning: NaN in outputs at batch 341\n",
      "Warning: NaN in outputs at batch 342\n",
      "Warning: NaN in outputs at batch 343\n",
      "Warning: NaN in outputs at batch 344\n",
      "Warning: NaN in outputs at batch 345\n",
      "Warning: NaN in outputs at batch 346\n",
      "Warning: NaN in outputs at batch 347\n",
      "Warning: NaN in outputs at batch 348\n",
      "Warning: NaN in outputs at batch 349\n",
      "Warning: NaN in outputs at batch 350\n",
      "Warning: NaN in outputs at batch 351\n",
      "Warning: NaN in outputs at batch 352\n",
      "Warning: NaN in outputs at batch 353\n",
      "Warning: NaN in outputs at batch 354\n",
      "Warning: NaN in outputs at batch 355\n",
      "Warning: NaN in outputs at batch 356\n",
      "Warning: NaN in outputs at batch 357\n",
      "Warning: NaN in outputs at batch 358\n",
      "Warning: NaN in outputs at batch 359\n",
      "Warning: NaN in outputs at batch 360\n",
      "Warning: NaN in outputs at batch 361\n",
      "Warning: NaN in outputs at batch 362\n",
      "Warning: NaN in outputs at batch 363\n",
      "Warning: NaN in outputs at batch 364\n",
      "Warning: NaN in outputs at batch 365\n",
      "Warning: NaN in outputs at batch 366\n",
      "Warning: NaN in outputs at batch 367\n",
      "Warning: NaN in outputs at batch 368\n",
      "Warning: NaN in outputs at batch 369\n",
      "Warning: NaN in outputs at batch 370\n",
      "Warning: NaN in outputs at batch 371\n",
      "Warning: NaN in outputs at batch 372\n",
      "Warning: NaN in outputs at batch 373\n",
      "Warning: NaN in outputs at batch 374\n",
      "Warning: NaN in outputs at batch 375\n",
      "Warning: NaN in outputs at batch 376\n",
      "Warning: NaN in outputs at batch 377\n",
      "Warning: NaN in outputs at batch 378\n",
      "Warning: NaN in outputs at batch 379\n",
      "Warning: NaN in outputs at batch 380\n",
      "Warning: NaN in outputs at batch 381\n",
      "Warning: NaN in outputs at batch 382\n",
      "Warning: NaN in outputs at batch 383\n",
      "Warning: NaN in outputs at batch 384\n",
      "Warning: NaN in outputs at batch 385\n",
      "Warning: NaN in outputs at batch 386\n",
      "\n",
      "Epoch 001 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Warning: NaN in outputs at batch 0\n",
      "Warning: NaN in outputs at batch 1\n",
      "Warning: NaN in outputs at batch 2\n",
      "Warning: NaN in outputs at batch 3\n",
      "Warning: NaN in outputs at batch 4\n",
      "Warning: NaN in outputs at batch 5\n",
      "Warning: NaN in outputs at batch 6\n",
      "Warning: NaN in outputs at batch 7\n",
      "Warning: NaN in outputs at batch 8\n",
      "Warning: NaN in outputs at batch 9\n",
      "Warning: NaN in outputs at batch 10\n",
      "Warning: NaN in outputs at batch 11\n",
      "Warning: NaN in outputs at batch 12\n",
      "Warning: NaN in outputs at batch 13\n",
      "Warning: NaN in outputs at batch 14\n",
      "Warning: NaN in outputs at batch 15\n",
      "Warning: NaN in outputs at batch 16\n",
      "Warning: NaN in outputs at batch 17\n",
      "Warning: NaN in outputs at batch 18\n",
      "Warning: NaN in outputs at batch 19\n",
      "Warning: NaN in outputs at batch 20\n",
      "Warning: NaN in outputs at batch 21\n",
      "Warning: NaN in outputs at batch 22\n",
      "Warning: NaN in outputs at batch 23\n",
      "Warning: NaN in outputs at batch 24\n",
      "Warning: NaN in outputs at batch 25\n",
      "Warning: NaN in outputs at batch 26\n",
      "Warning: NaN in outputs at batch 27\n",
      "Warning: NaN in outputs at batch 28\n",
      "Warning: NaN in outputs at batch 29\n",
      "Warning: NaN in outputs at batch 30\n",
      "Warning: NaN in outputs at batch 31\n",
      "Warning: NaN in outputs at batch 32\n",
      "Warning: NaN in outputs at batch 33\n",
      "Warning: NaN in outputs at batch 34\n",
      "Warning: NaN in outputs at batch 35\n",
      "Warning: NaN in outputs at batch 36\n",
      "Warning: NaN in outputs at batch 37\n",
      "Warning: NaN in outputs at batch 38\n",
      "Warning: NaN in outputs at batch 39\n",
      "Warning: NaN in outputs at batch 40\n",
      "Warning: NaN in outputs at batch 41\n",
      "Warning: NaN in outputs at batch 42\n",
      "Warning: NaN in outputs at batch 43\n",
      "Warning: NaN in outputs at batch 44\n",
      "Warning: NaN in outputs at batch 45\n",
      "Warning: NaN in outputs at batch 46\n",
      "Warning: NaN in outputs at batch 47\n",
      "Warning: NaN in outputs at batch 48\n",
      "Warning: NaN in outputs at batch 49\n",
      "Warning: NaN in outputs at batch 50\n",
      "Warning: NaN in outputs at batch 51\n",
      "Warning: NaN in outputs at batch 52\n",
      "Warning: NaN in outputs at batch 53\n",
      "Warning: NaN in outputs at batch 54\n",
      "Warning: NaN in outputs at batch 55\n",
      "Warning: NaN in outputs at batch 56\n",
      "Warning: NaN in outputs at batch 57\n",
      "Warning: NaN in outputs at batch 58\n",
      "Warning: NaN in outputs at batch 59\n",
      "Warning: NaN in outputs at batch 60\n",
      "Warning: NaN in outputs at batch 61\n",
      "Warning: NaN in outputs at batch 62\n",
      "Warning: NaN in outputs at batch 63\n",
      "Warning: NaN in outputs at batch 64\n",
      "Warning: NaN in outputs at batch 65\n",
      "Warning: NaN in outputs at batch 66\n",
      "Warning: NaN in outputs at batch 67\n",
      "Warning: NaN in outputs at batch 68\n",
      "Warning: NaN in outputs at batch 69\n",
      "Warning: NaN in outputs at batch 70\n",
      "Warning: NaN in outputs at batch 71\n",
      "Warning: NaN in outputs at batch 72\n",
      "Warning: NaN in outputs at batch 73\n",
      "Warning: NaN in outputs at batch 74\n",
      "Warning: NaN in outputs at batch 75\n",
      "Warning: NaN in outputs at batch 76\n",
      "Warning: NaN in outputs at batch 77\n",
      "Warning: NaN in outputs at batch 78\n",
      "Warning: NaN in outputs at batch 79\n",
      "Warning: NaN in outputs at batch 80\n",
      "Warning: NaN in outputs at batch 81\n",
      "Warning: NaN in outputs at batch 82\n",
      "Warning: NaN in outputs at batch 83\n",
      "Warning: NaN in outputs at batch 84\n",
      "Warning: NaN in outputs at batch 85\n",
      "Warning: NaN in outputs at batch 86\n",
      "Warning: NaN in outputs at batch 87\n",
      "Warning: NaN in outputs at batch 88\n",
      "Warning: NaN in outputs at batch 89\n",
      "Warning: NaN in outputs at batch 90\n",
      "Warning: NaN in outputs at batch 91\n",
      "Warning: NaN in outputs at batch 92\n",
      "Warning: NaN in outputs at batch 93\n",
      "Warning: NaN in outputs at batch 94\n",
      "Warning: NaN in outputs at batch 95\n",
      "Warning: NaN in outputs at batch 96\n",
      "Warning: NaN in outputs at batch 97\n",
      "Warning: NaN in outputs at batch 98\n",
      "Warning: NaN in outputs at batch 99\n",
      "Warning: NaN in outputs at batch 100\n",
      "Warning: NaN in outputs at batch 101\n",
      "Warning: NaN in outputs at batch 102\n",
      "Warning: NaN in outputs at batch 103\n",
      "Warning: NaN in outputs at batch 104\n",
      "Warning: NaN in outputs at batch 105\n",
      "Warning: NaN in outputs at batch 106\n",
      "Warning: NaN in outputs at batch 107\n",
      "Warning: NaN in outputs at batch 108\n",
      "Warning: NaN in outputs at batch 109\n",
      "Warning: NaN in outputs at batch 110\n",
      "Warning: NaN in outputs at batch 111\n",
      "Warning: NaN in outputs at batch 112\n",
      "Warning: NaN in outputs at batch 113\n",
      "Warning: NaN in outputs at batch 114\n",
      "Warning: NaN in outputs at batch 115\n",
      "Warning: NaN in outputs at batch 116\n",
      "Warning: NaN in outputs at batch 117\n",
      "Warning: NaN in outputs at batch 118\n",
      "Warning: NaN in outputs at batch 119\n",
      "Warning: NaN in outputs at batch 120\n",
      "Warning: NaN in outputs at batch 121\n",
      "Warning: NaN in outputs at batch 122\n",
      "Warning: NaN in outputs at batch 123\n",
      "Warning: NaN in outputs at batch 124\n",
      "Warning: NaN in outputs at batch 125\n",
      "Warning: NaN in outputs at batch 126\n",
      "Warning: NaN in outputs at batch 127\n",
      "Warning: NaN in outputs at batch 128\n",
      "Warning: NaN in outputs at batch 129\n",
      "Warning: NaN in outputs at batch 130\n",
      "Warning: NaN in outputs at batch 131\n",
      "Warning: NaN in outputs at batch 132\n",
      "Warning: NaN in outputs at batch 133\n",
      "Warning: NaN in outputs at batch 134\n",
      "Warning: NaN in outputs at batch 135\n",
      "Warning: NaN in outputs at batch 136\n",
      "Warning: NaN in outputs at batch 137\n",
      "Warning: NaN in outputs at batch 138\n",
      "Warning: NaN in outputs at batch 139\n",
      "Warning: NaN in outputs at batch 140\n",
      "Warning: NaN in outputs at batch 141\n",
      "Warning: NaN in outputs at batch 142\n",
      "Warning: NaN in outputs at batch 143\n",
      "Warning: NaN in outputs at batch 144\n",
      "Warning: NaN in outputs at batch 145\n",
      "Warning: NaN in outputs at batch 146\n",
      "Warning: NaN in outputs at batch 147\n",
      "Warning: NaN in outputs at batch 148\n",
      "Warning: NaN in outputs at batch 149\n",
      "Warning: NaN in outputs at batch 150\n",
      "Warning: NaN in outputs at batch 151\n",
      "Warning: NaN in outputs at batch 152\n",
      "Warning: NaN in outputs at batch 153\n",
      "Warning: NaN in outputs at batch 154\n",
      "Warning: NaN in outputs at batch 155\n",
      "Warning: NaN in outputs at batch 156\n",
      "Warning: NaN in outputs at batch 157\n",
      "Warning: NaN in outputs at batch 158\n",
      "Warning: NaN in outputs at batch 159\n",
      "Warning: NaN in outputs at batch 160\n",
      "Warning: NaN in outputs at batch 161\n",
      "Warning: NaN in outputs at batch 162\n",
      "Warning: NaN in outputs at batch 163\n",
      "Warning: NaN in outputs at batch 164\n",
      "Warning: NaN in outputs at batch 165\n",
      "Warning: NaN in outputs at batch 166\n",
      "Warning: NaN in outputs at batch 167\n",
      "Warning: NaN in outputs at batch 168\n",
      "Warning: NaN in outputs at batch 169\n",
      "Warning: NaN in outputs at batch 170\n",
      "Warning: NaN in outputs at batch 171\n",
      "Warning: NaN in outputs at batch 172\n",
      "Warning: NaN in outputs at batch 173\n",
      "Warning: NaN in outputs at batch 174\n",
      "Warning: NaN in outputs at batch 175\n",
      "Warning: NaN in outputs at batch 176\n",
      "Warning: NaN in outputs at batch 177\n",
      "Warning: NaN in outputs at batch 178\n",
      "Warning: NaN in outputs at batch 179\n",
      "Warning: NaN in outputs at batch 180\n",
      "Warning: NaN in outputs at batch 181\n",
      "Warning: NaN in outputs at batch 182\n",
      "Warning: NaN in outputs at batch 183\n",
      "Warning: NaN in outputs at batch 184\n",
      "Warning: NaN in outputs at batch 185\n",
      "Warning: NaN in outputs at batch 186\n",
      "Warning: NaN in outputs at batch 187\n",
      "Warning: NaN in outputs at batch 188\n",
      "Warning: NaN in outputs at batch 189\n",
      "Warning: NaN in outputs at batch 190\n",
      "Warning: NaN in outputs at batch 191\n",
      "Warning: NaN in outputs at batch 192\n",
      "Warning: NaN in outputs at batch 193\n",
      "Warning: NaN in outputs at batch 194\n",
      "Warning: NaN in outputs at batch 195\n",
      "Warning: NaN in outputs at batch 196\n",
      "Warning: NaN in outputs at batch 197\n",
      "Warning: NaN in outputs at batch 198\n",
      "Warning: NaN in outputs at batch 199\n",
      "Warning: NaN in outputs at batch 200\n",
      "Warning: NaN in outputs at batch 201\n",
      "Warning: NaN in outputs at batch 202\n",
      "Warning: NaN in outputs at batch 203\n",
      "Warning: NaN in outputs at batch 204\n",
      "Warning: NaN in outputs at batch 205\n",
      "Warning: NaN in outputs at batch 206\n",
      "Warning: NaN in outputs at batch 207\n",
      "Warning: NaN in outputs at batch 208\n",
      "Warning: NaN in outputs at batch 209\n",
      "Warning: NaN in outputs at batch 210\n",
      "Warning: NaN in outputs at batch 211\n",
      "Warning: NaN in outputs at batch 212\n",
      "Warning: NaN in outputs at batch 213\n",
      "Warning: NaN in outputs at batch 214\n",
      "Warning: NaN in outputs at batch 215\n",
      "Warning: NaN in outputs at batch 216\n",
      "Warning: NaN in outputs at batch 217\n",
      "Warning: NaN in outputs at batch 218\n",
      "Warning: NaN in outputs at batch 219\n",
      "Warning: NaN in outputs at batch 220\n",
      "Warning: NaN in outputs at batch 221\n",
      "Warning: NaN in outputs at batch 222\n",
      "Warning: NaN in outputs at batch 223\n",
      "Warning: NaN in outputs at batch 224\n",
      "Warning: NaN in outputs at batch 225\n",
      "Warning: NaN in outputs at batch 226\n",
      "Warning: NaN in outputs at batch 227\n",
      "Warning: NaN in outputs at batch 228\n",
      "Warning: NaN in outputs at batch 229\n",
      "Warning: NaN in outputs at batch 230\n",
      "Warning: NaN in outputs at batch 231\n",
      "Warning: NaN in outputs at batch 232\n",
      "Warning: NaN in outputs at batch 233\n",
      "Warning: NaN in outputs at batch 234\n",
      "Warning: NaN in outputs at batch 235\n",
      "Warning: NaN in outputs at batch 236\n",
      "Warning: NaN in outputs at batch 237\n",
      "Warning: NaN in outputs at batch 238\n",
      "Warning: NaN in outputs at batch 239\n",
      "Warning: NaN in outputs at batch 240\n",
      "Warning: NaN in outputs at batch 241\n",
      "Warning: NaN in outputs at batch 242\n",
      "Warning: NaN in outputs at batch 243\n",
      "Warning: NaN in outputs at batch 244\n",
      "Warning: NaN in outputs at batch 245\n",
      "Warning: NaN in outputs at batch 246\n",
      "Warning: NaN in outputs at batch 247\n",
      "Warning: NaN in outputs at batch 248\n",
      "Warning: NaN in outputs at batch 249\n",
      "Warning: NaN in outputs at batch 250\n",
      "Warning: NaN in outputs at batch 251\n",
      "Warning: NaN in outputs at batch 252\n",
      "Warning: NaN in outputs at batch 253\n",
      "Warning: NaN in outputs at batch 254\n",
      "Warning: NaN in outputs at batch 255\n",
      "Warning: NaN in outputs at batch 256\n",
      "Warning: NaN in outputs at batch 257\n",
      "Warning: NaN in outputs at batch 258\n",
      "Warning: NaN in outputs at batch 259\n",
      "Warning: NaN in outputs at batch 260\n",
      "Warning: NaN in outputs at batch 261\n",
      "Warning: NaN in outputs at batch 262\n",
      "Warning: NaN in outputs at batch 263\n",
      "Warning: NaN in outputs at batch 264\n",
      "Warning: NaN in outputs at batch 265\n",
      "Warning: NaN in outputs at batch 266\n",
      "Warning: NaN in outputs at batch 267\n",
      "Warning: NaN in outputs at batch 268\n",
      "Warning: NaN in outputs at batch 269\n",
      "Warning: NaN in outputs at batch 270\n",
      "Warning: NaN in outputs at batch 271\n",
      "Warning: NaN in outputs at batch 272\n",
      "Warning: NaN in outputs at batch 273\n",
      "Warning: NaN in outputs at batch 274\n",
      "Warning: NaN in outputs at batch 275\n",
      "Warning: NaN in outputs at batch 276\n",
      "Warning: NaN in outputs at batch 277\n",
      "Warning: NaN in outputs at batch 278\n",
      "Warning: NaN in outputs at batch 279\n",
      "Warning: NaN in outputs at batch 280\n",
      "Warning: NaN in outputs at batch 281\n",
      "Warning: NaN in outputs at batch 282\n",
      "Warning: NaN in outputs at batch 283\n",
      "Warning: NaN in outputs at batch 284\n",
      "Warning: NaN in outputs at batch 285\n",
      "Warning: NaN in outputs at batch 286\n",
      "Warning: NaN in outputs at batch 287\n",
      "Warning: NaN in outputs at batch 288\n",
      "Warning: NaN in outputs at batch 289\n",
      "Warning: NaN in outputs at batch 290\n",
      "Warning: NaN in outputs at batch 291\n",
      "Warning: NaN in outputs at batch 292\n",
      "Warning: NaN in outputs at batch 293\n",
      "Warning: NaN in outputs at batch 294\n",
      "Warning: NaN in outputs at batch 295\n",
      "Warning: NaN in outputs at batch 296\n",
      "Warning: NaN in outputs at batch 297\n",
      "Warning: NaN in outputs at batch 298\n",
      "Warning: NaN in outputs at batch 299\n",
      "Warning: NaN in outputs at batch 300\n",
      "Warning: NaN in outputs at batch 301\n",
      "Warning: NaN in outputs at batch 302\n",
      "Warning: NaN in outputs at batch 303\n",
      "Warning: NaN in outputs at batch 304\n",
      "Warning: NaN in outputs at batch 305\n",
      "Warning: NaN in outputs at batch 306\n",
      "Warning: NaN in outputs at batch 307\n",
      "Warning: NaN in outputs at batch 308\n",
      "Warning: NaN in outputs at batch 309\n",
      "Warning: NaN in outputs at batch 310\n",
      "Warning: NaN in outputs at batch 311\n",
      "Warning: NaN in outputs at batch 312\n",
      "Warning: NaN in outputs at batch 313\n",
      "Warning: NaN in outputs at batch 314\n",
      "Warning: NaN in outputs at batch 315\n",
      "Warning: NaN in outputs at batch 316\n",
      "Warning: NaN in outputs at batch 317\n",
      "Warning: NaN in outputs at batch 318\n",
      "Warning: NaN in outputs at batch 319\n",
      "Warning: NaN in outputs at batch 320\n",
      "Warning: NaN in outputs at batch 321\n",
      "Warning: NaN in outputs at batch 322\n",
      "Warning: NaN in outputs at batch 323\n",
      "Warning: NaN in outputs at batch 324\n",
      "Warning: NaN in outputs at batch 325\n",
      "Warning: NaN in outputs at batch 326\n",
      "Warning: NaN in outputs at batch 327\n",
      "Warning: NaN in outputs at batch 328\n",
      "Warning: NaN in outputs at batch 329\n",
      "Warning: NaN in outputs at batch 330\n",
      "Warning: NaN in outputs at batch 331\n",
      "Warning: NaN in outputs at batch 332\n",
      "Warning: NaN in outputs at batch 333\n",
      "Warning: NaN in outputs at batch 334\n",
      "Warning: NaN in outputs at batch 335\n",
      "Warning: NaN in outputs at batch 336\n",
      "Warning: NaN in outputs at batch 337\n",
      "Warning: NaN in outputs at batch 338\n",
      "Warning: NaN in outputs at batch 339\n",
      "Warning: NaN in outputs at batch 340\n",
      "Warning: NaN in outputs at batch 341\n",
      "Warning: NaN in outputs at batch 342\n",
      "Warning: NaN in outputs at batch 343\n",
      "Warning: NaN in outputs at batch 344\n",
      "Warning: NaN in outputs at batch 345\n",
      "Warning: NaN in outputs at batch 346\n",
      "Warning: NaN in outputs at batch 347\n",
      "Warning: NaN in outputs at batch 348\n",
      "Warning: NaN in outputs at batch 349\n",
      "Warning: NaN in outputs at batch 350\n",
      "Warning: NaN in outputs at batch 351\n",
      "Warning: NaN in outputs at batch 352\n",
      "Warning: NaN in outputs at batch 353\n",
      "Warning: NaN in outputs at batch 354\n",
      "Warning: NaN in outputs at batch 355\n",
      "Warning: NaN in outputs at batch 356\n",
      "Warning: NaN in outputs at batch 357\n",
      "Warning: NaN in outputs at batch 358\n",
      "Warning: NaN in outputs at batch 359\n",
      "Warning: NaN in outputs at batch 360\n",
      "Warning: NaN in outputs at batch 361\n",
      "Warning: NaN in outputs at batch 362\n",
      "Warning: NaN in outputs at batch 363\n",
      "Warning: NaN in outputs at batch 364\n",
      "Warning: NaN in outputs at batch 365\n",
      "Warning: NaN in outputs at batch 366\n",
      "Warning: NaN in outputs at batch 367\n",
      "Warning: NaN in outputs at batch 368\n",
      "Warning: NaN in outputs at batch 369\n",
      "Warning: NaN in outputs at batch 370\n",
      "Warning: NaN in outputs at batch 371\n",
      "Warning: NaN in outputs at batch 372\n",
      "Warning: NaN in outputs at batch 373\n",
      "Warning: NaN in outputs at batch 374\n",
      "Warning: NaN in outputs at batch 375\n",
      "Warning: NaN in outputs at batch 376\n",
      "Warning: NaN in outputs at batch 377\n",
      "Warning: NaN in outputs at batch 378\n",
      "Warning: NaN in outputs at batch 379\n",
      "Warning: NaN in outputs at batch 380\n",
      "Warning: NaN in outputs at batch 381\n",
      "Warning: NaN in outputs at batch 382\n",
      "Warning: NaN in outputs at batch 383\n",
      "Warning: NaN in outputs at batch 384\n",
      "Warning: NaN in outputs at batch 385\n",
      "Warning: NaN in outputs at batch 386\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 50\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\QT\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\QT\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\QT\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\QT\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\QT\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:211\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransposed\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\QT\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\QT\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\QT\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # 检查输出是否有效\n",
    "        if torch.isnan(outputs).any():\n",
    "            print(f\"Warning: NaN in outputs at batch {batch_idx}\")\n",
    "            continue\n",
    "            \n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 检查损失是否有效\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"Warning: NaN loss at batch {batch_idx}\")\n",
    "            continue\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        # 梯度裁剪\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # 检查梯度\n",
    "        valid_gradients = True\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                if torch.isnan(param.grad).any():\n",
    "                    print(f\"Warning: NaN gradient in {name}\")\n",
    "                    valid_gradients = False\n",
    "                    break\n",
    "        \n",
    "        if valid_gradients:\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f'Epoch: {epoch+1}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # 验证阶段\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # 检查输出是否有效\n",
    "            if torch.isnan(outputs).any():\n",
    "                continue\n",
    "                \n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # 检查损失是否有效\n",
    "            if not torch.isnan(loss):\n",
    "                val_loss += loss.item()\n",
    "    \n",
    "    # 计算平均损失\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    print(f'\\nEpoch {epoch+1:03d} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    # 更新学习率\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    # 保存历史\n",
    "    history['train'].append(avg_train_loss)\n",
    "    history['val'].append(avg_val_loss)\n",
    "    \n",
    "    # 早停检查\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': best_val_loss,\n",
    "        }, 'best_model.pth')\n",
    "        no_improve_count = 0\n",
    "    else:\n",
    "        no_improve_count += 1\n",
    "        \n",
    "    if no_improve_count >= early_stopping_patience:\n",
    "        print(f'\\nEarly stopping triggered after {epoch+1} epochs')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.训练可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['train'], label='Training Loss')\n",
    "plt.plot(history['val'], label='Validation Loss')\n",
    "plt.title('Training Progress')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评估过程中出错: Error(s) in loading state_dict for ImprovedBiLSTMAttention:\n",
      "\tMissing key(s) in state_dict: \"regressor.0.weight\", \"regressor.0.bias\", \"regressor.2.weight\", \"regressor.2.bias\", \"regressor.2.running_mean\", \"regressor.2.running_var\", \"regressor.4.weight\", \"regressor.4.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"classifier.0.weight\", \"classifier.0.bias\", \"classifier.2.weight\", \"classifier.2.bias\", \"classifier.2.running_mean\", \"classifier.2.running_var\", \"classifier.2.num_batches_tracked\", \"classifier.4.weight\", \"classifier.4.bias\". \n",
      "\n",
      "调试信息:\n",
      "训练集样本数: 24768\n",
      "验证集样本数: 6144\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False    # 用来正常显示负号\n",
    "def evaluate_detailed(model, data_loader, device, name=\"\"):\n",
    "    \"\"\"\n",
    "    详细评估模型性能，包括回归指标\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # 获取模型输出\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # 收集预测结果\n",
    "            all_predictions.extend(outputs.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    # 转换为numpy数组\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_targets = np.array(all_targets)\n",
    "    \n",
    "    # 计算评估指标\n",
    "    mae = mean_absolute_error(all_targets, all_predictions)\n",
    "    mse = mean_squared_error(all_targets, all_predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    corr = np.corrcoef(all_predictions, all_targets)[0, 1] if len(all_predictions) > 1 else 0\n",
    "    \n",
    "    # 计算方向准确率\n",
    "    pred_direction = np.sign(all_predictions - np.mean(all_predictions))\n",
    "    actual_direction = np.sign(all_targets - np.mean(all_targets))\n",
    "    direction_accuracy = 100 * np.mean(pred_direction == actual_direction)\n",
    "    \n",
    "    # 打印结果\n",
    "    print(f\"\\n{name}结果:\")\n",
    "    print(f\"- 样本数量: {len(all_targets)}\")\n",
    "    print(f\"- MAE: {mae:.4f}\")\n",
    "    print(f\"- RMSE: {rmse:.4f}\")\n",
    "    print(f\"- 相关系数: {corr:.4f}\")\n",
    "    print(f\"- 方向准确率: {direction_accuracy:.2f}%\")\n",
    "    \n",
    "    return all_predictions, all_targets, mae, corr, direction_accuracy, rmse\n",
    "\n",
    "# 加载最佳模型并评估\n",
    "try:\n",
    "    # 加载检查点\n",
    "    checkpoint = torch.load('best_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    best_epoch = checkpoint['epoch']\n",
    "    best_loss = checkpoint['loss']\n",
    "    print(f\"\\n加载最佳模型 (epoch {best_epoch+1}, loss {best_loss:.4f})\")\n",
    "    \n",
    "    # 评估训练集和验证集\n",
    "    print(\"\\n=== 最佳模型评估 ===\")\n",
    "    train_preds, train_acts, train_mae, train_corr, train_dir_acc, train_class_acc = evaluate_detailed(\n",
    "    model, train_loader, device, \"训练集\")\n",
    "\n",
    "    val_preds, val_acts, val_mae, val_corr, val_dir_acc, val_class_acc = evaluate_detailed(\n",
    "    model, val_loader, device, \"验证集\")\n",
    "    \n",
    "    # 绘制预测结果\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # 训练集预测vs实际值\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(train_acts, train_preds, alpha=0.5, label='预测点')\n",
    "    plt.plot([train_acts.min(), train_acts.max()], \n",
    "             [train_acts.min(), train_acts.max()], \n",
    "             'r--', label='完美预测线')\n",
    "    plt.title(f'训练集: 预测值 vs 实际值\\nMAE: {train_mae:.4f}, 相关系数: {train_corr:.4f}')\n",
    "    plt.xlabel('实际值')\n",
    "    plt.ylabel('预测值')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 验证集预测vs实际值\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(val_acts, val_preds, alpha=0.5, label='预测点')\n",
    "    plt.plot([val_acts.min(), val_acts.max()], \n",
    "             [val_acts.min(), val_acts.max()], \n",
    "             'r--', label='完美预测线')\n",
    "    plt.title(f'验证集: 预测值 vs 实际值\\nMAE: {val_mae:.4f}, 相关系数: {val_corr:.4f}')\n",
    "    plt.xlabel('实际值')\n",
    "    plt.ylabel('预测值')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 打印详细统计信息\n",
    "    print(\"\\n=== 详细统计信息 ===\")\n",
    "    print(f\"训练集预测范围: [{train_preds.min():.4f}, {train_preds.max():.4f}]\")\n",
    "    print(f\"训练集实际范围: [{train_acts.min():.4f}, {train_acts.max():.4f}]\")\n",
    "    print(f\"验证集预测范围: [{val_preds.min():.4f}, {val_preds.max():.4f}]\")\n",
    "    print(f\"验证集实际范围: [{val_acts.min():.4f}, {val_acts.max():.4f}]\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"评估过程中出错: {e}\")\n",
    "    print(\"\\n调试信息:\")\n",
    "    for name, loader in [(\"训练集\", train_loader), (\"验证集\", val_loader)]:\n",
    "        total_samples = sum(len(targets) for inputs, targets in loader)\n",
    "        print(f\"{name}样本数: {total_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.结果可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_acts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(\u001b[43mtrain_acts\u001b[49m, train_preds, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m训练集预测 vs 实际\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m实际值\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_acts' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAGvCAYAAAB1pf5FAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAGjVJREFUeJzt3QFsVeXdwOG3UKEyaYXihiJghzrFjZAQlLnJ4oZhiriZLXFoFtQwjZM5YkIiOjUqAZ2KQ7PpJgyM4kycUdEFt+nE6SRbIIwJCGauSJWgdUApYlDhfjnnWxsQWu4tbfnbPk9yMk97Tu8h7+j9ce773ltWKBQKCQAgoB6H+wIAAFoiVACAsIQKABCWUAEAwhIqAEBYQgUACEuoAABhCRUAICyhAgCEJVQAgK4TKu+//36qqalJGzZsKOr4l156KZ166qlpwIABac6cOW25RgCgm+pRaqScf/75RUdKfX19uuCCC9KkSZPSsmXL0qJFi9KLL77Y1msFALqZ8lIO/sEPfpAuvvji9Pe//72o47MwOe6449KNN96YysrK0k033ZTmz5+fzj777AMev2vXrnxrsmfPnrRly5ZUXV2dnw8AxJV9znFjY2P+3N+jRzvNLimU4D//+U/+v9lptbW1Bz3+0ksvLVx11VXN+5s2bSqccsopLR5/88035z/bZrPZbDbbZ3erq6srtJeS7qhkc1NKsX379jR8+PDm/crKyrRp06YWj58xY0a69tprm/cbGhrSkCFDUl1dXX4uABBX9rw/ePDg1Ldv33b7mSWFSsk/vLw89e7du3m/oqIi7dy5s8Xjs2P3Pr5JFilCBQA+G9pzukaHLk/u379/PqG2Sfa6Va9evTryIQGALqRDQ2X06NH5ap8mK1euTIMGDerIhwQAupAe7fWa1Mcff7zf17OlyX/729/S888/n3//5z//eRo/fnx7PCQA0A20S6iMGDEi/eEPf9jv69mbvN1zzz3pvPPOS1/4whfS+vXr089+9rP2eEgAoBsoy5b+dPSD1NbWpnXr1qWzzjorHXXUUSXdqamqqspX/5hMCwCxdcTzdoeu+tl7WXOpS5sBAHwoIQAQllABAMISKgBAWEIFAAhLqAAAYQkVACAsoQIAhCVUAICwhAoAEJZQAQDCEioAQFhCBQAIS6gAAGEJFQAgLKECAIQlVACAsIQKABCWUAEAwhIqAEBYQgUACEuoAABhCRUAICyhAgCEJVQAgLCECgAQllABAMISKgBAWEIFAAhLqAAAYQkVACAsoQIAhCVUAICwhAoAEJZQAQDCEioAQFhCBQAIS6gAAGEJFQAgLKECAIQlVACAsIQKABCWUAEAwhIqAEBYQgUACEuoAABhCRUAICyhAgCEJVQAgLCECgAQllABAMISKgBAWEIFAAhLqAAAYQkVACAsoQIAhCVUAICwhAoAEJZQAQDCEioAQFhCBQAIS6gAAGEJFQAgLKECAIQlVACAsIQKANA1QmX16tVp9OjRqV+/fmn69OmpUCi0enz2/auuuir1798/HX300enSSy9NH3744aFeMwDQTRQdKrt27UoTJ05Mo0aNSsuXL09r165NCxcubPWchx9+OK1fvz6tXLkyvfzyy2nNmjVp9uzZ7XHdAEA3UHSoLFmyJDU0NKQ5c+akYcOGpVmzZqX58+e3es4//vGP9P3vfz8NHTo0feUrX0nf/e5307///e9WY2j79u37bABA91V0qKxatSqNGTMm9enTJ98fMWJEflelNaeddlp65JFH0rvvvpveeuut9Nhjj6VzzjmnxeOzuy1VVVXN2+DBg0v5swAA3TVUsrsbNTU1zftlZWWpZ8+eaevWrS2eM2XKlLRjx440cODAdMIJJ+TnT548ucXjZ8yYkd+1adrq6upK+bMAAN01VMrLy1Pv3r33+VpFRUXauXNni+fMnTs3n0Sb3U3ZuHFj+uSTT/JJuC3Jfn5lZeU+GwDQfRUdKtnKnfr6+n2+1tjYmHr16tXiOYsWLcrDZMiQIfnLONlLOweb1wIAUHKoZMuSly1b1rxfW1ubT37NAqYle/bsSe+9917z/ubNm9Pu3buLfUgAoJsrL/bAsWPH5vNUFixYkC677LJ81c+4cePyeSrbtm1Lffv2zf97b2eddVa6/fbb869/9NFH6Y477kgXXHBBR/w5AIAuqKxwsHdt28vixYvTpEmT0pFHHpl69OiRli5dmoYPH55PrM3eK2XkyJH7HJ8FzDXXXJOee+65/GWi8ePHp3nz5qUBAwYU9XhZGGWrf7KJtearAEBsHfG8XVKoNL18s2LFinypcnV1depIQgUAPjs64nm76Jd+mmRLjSdMmNAuDw4A0BofSggAhCVUAICwhAoAEJZQAQDCEioAQFhCBQAIS6gAAGEJFQAgLKECAIQlVACAsIQKABCWUAEAwhIqAEBYQgUACEuoAABhCRUAICyhAgCEJVQAgLCECgAQllABAMISKgBAWEIFAAhLqAAAYQkVACAsoQIAhCVUAICwhAoAEJZQAQDCEioAQFhCBQAIS6gAAGEJFQAgLKECAIQlVACAsIQKABCWUAEAwhIqAEBYQgUACEuoAABhCRUAICyhAgCEJVQAgLCECgAQllABAMISKgBAWEIFAAhLqAAAYQkVACAsoQIAhCVUAICwhAoAEJZQAQDCEioAQFhCBQAIS6gAAGEJFQAgLKECAIQlVACAsIQKABCWUAEAwhIqAEBYQgUACEuoAABhCRUAICyhAgB0jVBZvXp1Gj16dOrXr1+aPn16KhQKRZ23Z8+edOaZZ6a77767rdcJAHRDRYfKrl270sSJE9OoUaPS8uXL09q1a9PChQuLOveBBx5IDQ0N6ZprrjmUawUAupmiQ2XJkiV5bMyZMycNGzYszZo1K82fP/+g523atCldf/316b777ktHHHHEQWNo+/bt+2wAQPdVdKisWrUqjRkzJvXp0yffHzFiRH5X5WCmTZuWhg4dmurq6tKrr77a6rGzZ89OVVVVzdvgwYOLvTwAoDuHSnZ3o6ampnm/rKws9ezZM23durXFc5YtW5Yef/zxdPzxx6c333wzTZ48OU2dOrXF42fMmJHftWnasrgBALqv8qIPLC9PvXv33udrFRUVaefOnfnk2gN58MEH0xlnnJGeffbZPGx+9KMf5XdXfvKTn6QvfelL+x2f/fxPPwYA0H0VfUelf//+qb6+fp+vNTY2pl69erV4zttvv53OO++8PFIy2Us5xxxzTH53BQCg3UIlW5acvZTTpLa2Np/8mgVMS7KXfD788MPm/R07dqQtW7akQYMGFfuwAEA3VnSojB07Np+nsmDBgnw/W/Uzbty4fJ7Ktm3b0u7du/c7Z9KkSfnLPy+88EJ666230o9//ON0yimn5BNxAQAOpqxQ7Lu2pZQWL16cx8eRRx6ZevTokZYuXZqGDx+ev7SzcuXKNHLkyP3OyZYw33HHHfnE2Oz72XuvHGh+yoFkYZSt/skm1lZWVhZ7mQDAYdARz9slhUpm8+bNacWKFflS5erq6tSRhAoAfHZ0xPN20at+mgwcODBNmDChXR4cAKA1PpQQAAhLqAAAYQkVACAsoQIAhCVUAICwhAoAEJZQAQDCEioAQFhCBQAIS6gAAGEJFQAgLKECAIQlVACAsIQKABCWUAEAwhIqAEBYQgUACEuoAABhCRUAICyhAgCEJVQAgLCECgAQllABAMISKgBAWEIFAAhLqAAAYQkVACAsoQIAhCVUAICwhAoAEJZQAQDCEioAQFhCBQAIS6gAAGEJFQAgLKECAIQlVACAsIQKABCWUAEAwhIqAEBYQgUACEuoAABhCRUAICyhAgCEJVQAgLCECgAQllABAMISKgBAWEIFAAhLqAAAYQkVACAsoQIAhCVUAICwhAoAEJZQAQDCEioAQFhCBQAIS6gAAGEJFQAgLKECAIQlVACAsIQKABCWUAEAwhIqAEBYQgUA6Dqhsnr16jR69OjUr1+/NH369FQoFIo+d9u2benYY49NGzZsKPVhAYBuqKRQ2bVrV5o4cWIaNWpUWr58eVq7dm1auHBh0ednYbN58+a2XCcA0A2VFCpLlixJDQ0Nac6cOWnYsGFp1qxZaf78+UWd+9e//jUtXrw4VVdXtxpC27dv32cDALqvkkJl1apVacyYMalPnz75/ogRI/K7KgeTBciVV16Z7r333nTUUUe1eNzs2bNTVVVV8zZ48OBSLg8A6M6hkt3hqKmpad4vKytLPXv2TFu3bm31vOzOy8knn5wuuuiiVo+bMWNGfsemaaurqyvl8gCALqa8pIPLy1Pv3r33+VpFRUXauXNnPrn2QF5//fX0wAMPpJUrVx7052c/+9M/HwDovkq6o9K/f/9UX1+/z9caGxtTr169Dnh8tiLoiiuuSDNnzkzHHXfcoV0pANDtlBQq2bLkZcuWNe/X1tbm80+ygDmQjRs3pldeeSVf7XP00UfnW/a1bG7Lo48+euhXDwB0aSW99DN27Nh8nsqCBQvSZZddls89GTduXD5PJXuPlL59++b/3WTQoEF5zOzt61//enrsscfSyJEj2+9PAQB0SSXPUZk3b16aNGlSfpekR48eaenSpfn3sjkq2TyUvQMkO/6EE07Y72ccf/zxra7+AQDIlBVKeWvZ/8netG3FihX5UuXW3hflUGV3b7JlytkKoMrKyg57HAAg5vN2SXdUmgwcODBNmDChXS4AAKAlPpQQAAhLqAAAYQkVACAsoQIAhCVUAICwhAoAEJZQAQDCEioAQFhCBQAIS6gAAGEJFQAgLKECAIQlVACAsIQKABCWUAEAwhIqAEBYQgUACEuoAABhCRUAICyhAgCEJVQAgLCECgAQllABAMISKgBAWEIFAAhLqAAAYQkVACAsoQIAhCVUAICwhAoAEJZQAQDCEioAQFhCBQAIS6gAAGEJFQAgLKECAIQlVACAsIQKABCWUAEAwhIqAEBYQgUACEuoAABhCRUAICyhAgCEJVQAgLCECgAQllABAMISKgBAWEIFAAhLqAAAYQkVACAsoQIAhCVUAICwhAoAEJZQAQDCEioAQFhCBQAIS6gAAGEJFQAgLKECAIQlVACAsIQKABCWUAEAwhIqAEBYQgUA6Bqhsnr16jR69OjUr1+/NH369FQoFA56zi233JL69++fevfunS688MLU2Nh4KNcLAHQjRYfKrl270sSJE9OoUaPS8uXL09q1a9PChQtbPWfRokX59txzz6U1a9ak119/Pd1+++3tcd0AQDdQdKgsWbIkNTQ0pDlz5qRhw4alWbNmpfnz57d6Tl1dXXrooYfS6aefnk488cR00UUXpZUrV7YaQ9u3b99nAwC6r/JiD1y1alUaM2ZM6tOnT74/YsSI/K5Ka6677rp99tevX59OOumkFo+fPXt2/lIRAEBJd1Syuxs1NTXN+2VlZalnz55p69atRZ3/xhtvpCeffDJdccUVLR4zY8aM/K5N05bdkQEAuq+i76iUl5fnE2L3VlFRkXbu3JlPrm3Nnj170uWXX56mTJmSTjvttBaPy37+px8DAOi+ir6jkq3cqa+v3+dr2QqeXr16HfTc2267LW3ZsiXdeeedbbtKAKBbKjpUsmXJy5Yta96vra3NJ79mAdOaZ555Jp+A+8QTTzTPbwEAaNdQGTt2bD5PZcGCBfl+tupn3Lhx+TyVbdu2pd27d+93TrYcedKkSem+++5LgwcPTjt27MhfKgIAaNdQyeaozJs3L02dOjUNGDAgPf300+mOO+7Iv5fNUXnttdf2O+c3v/lN+uCDD9LkyZNT375982348OHFPiQA0M2VFYp5e9m9bN68Oa1YsSJfqlxdXd1xV/a/lUZVVVX5CqDKysoOfSwAIN7zdtGrfpoMHDgwTZgwoV0eHACgNT6UEAAIS6gAAGEJFQAgLKECAIQlVACAsIQKABCWUAEAwhIqAEBYQgUACEuoAABhCRUAICyhAgCEJVQAgLCECgAQllABAMISKgBAWEIFAAhLqAAAYQkVACAsoQIAhCVUAICwhAoAEJZQAQDCEioAQFhCBQAIS6gAAGEJFQAgLKECAIQlVACAsIQKABCWUAEAwhIqAEBYQgUACEuoAABhCRUAICyhAgCEJVQAgLCECgAQllABAMISKgBAWEIFAAhLqAAAYQkVACAsoQIAhCVUAICwhAoAEJZQAQDCEioAQFhCBQAIS6gAAGEJFQAgLKECAIQlVACAsIQKABCWUAEAwhIqAEBYQgUACEuoAABhCRUAICyhAgCEJVQAgLCECgAQllABAMISKgBAWEIFAOg6obJ69eo0evTo1K9fvzR9+vRUKBQOes7vf//7NHTo0HTcccel3/3ud229VgCgmykpVHbt2pUmTpyYRo0alZYvX57Wrl2bFi5ceNCwueSSS9KNN96Y/vjHP6abbroprV+//lCvGwDoBsoKxdwS+Z+nnnoqXX755entt99Offr0SatWrUpXX311euWVV1o8Z9q0aWndunXpueeey/fnzp2b6uvr08yZMw8YQtnWpKGhIQ0ZMiTV1dWlysrK0v90AECn2b59exo8eHDatm1bqqqqapefWV7KwVmYjBkzJo+UzIgRI/K7Kgc759xzz23eP/3009Ott956wGNnz56dbrnllv2+nv2hAYDPhv/+97+HJ1SyUqqpqWneLysrSz179kxbt27N56wUc052Z2TTpk0HPHbGjBnp2muvbd7Piiyb27Jx48Z2+wNzaJXs7tbhZyxiMA5xGIs4ml4J6d+/f7v9zJJCpby8PPXu3Xufr1VUVKSdO3e2GCqfPqfp+APJjvv0z89kkeL/fDFk42AsYjAWMRiHOIxFHD16tN+i4pJ+UlZI2fySvTU2NqZevXoVfc7BjgcAaFOoZMuSly1b1rxfW1ubT35t7RbPp89ZuXJlGjRoUCkPCwB0UyWFytixY/PXAhcsWJDvz5o1K40bNy6fp5LNJ9m9e/d+53zve99Ljz32WHrttdfSjh070r333pvGjx9f1ONlLwPdfPPNB3w5iM5lLOIwFjEYhziMRdcei5KWJ2cWL16cJk2alI488sj8NailS5em4cOH5xNrs7slI0eO3O+cG264Id111135/JSTTjopvfzyy/n5AADtGiqZzZs3pxUrVuRLlaurq4s6J1vG/M4776RvfOMb5qgAAB0XKgAAncGHEgIAYQkVACCswx4qPo05hraMQ/ZxB9nS9Gx294UXXpi/Rw6HZyyaZKvvjj322LRhw4YOvcbuoK3jsGfPnnTmmWemu+++u8OvsbsodSyy71911VX576ejjz46XXrppenDDz/stOvtyt5///383eaL/R3z0ksvpVNPPTUNGDAgzZkz57MXKj6NOYa2jMOiRYvyLfuwyTVr1qTXX3893X777Z12zV1VW8Zib9kv8WyyO4dvHB544IH8bcSvueaaDr/O7qAtY/Hwww/nzwvZStRslWn2Oyr7LDkOPVLOP//8oiMle7PXCy64IF8pnL2fWvac8eKLL5b+wIXD6Mknnyz069ev8MEHH+T7//znPwtf+9rXWj3npz/9aWH8+PHN+7/4xS8KN9xwQ4dfa1fWlnGYPXt24dVXX23ev+mmmwrnnntuh19rV9eWsWjy0ksvFT7/+c8XqqurC7W1tR18pV1bW8fhnXfeKVRVVRVeeOGFTrjK7qEtY3H11VcXfvnLXzbvz5w5szBp0qQOv9au7lvf+lZh7ty52e2son7H3HPPPYVTTjmlsGfPnnz/qaeeKlxyySUlP+5hvaPS1k9j/uY3v7nPpzFnS6Xp3HG47rrr0le/+tXm/exfL9l75ND5Y9H0r84rr7wyf0PFo446qhOutGtr6zhMmzYtf1k6+3C8V199tROutOtry1icdtpp6ZFHHknvvvtueuutt/I3HT3nnHM66Yq7rgcffLCkO4XZ2J199tn5+6wdyvP1YQ2V1j6NudhzWvs0ZjpuHPb2xhtvpCeffDJdccUVHXiV3UNbxyJ7l+iTTz45XXTRRZ1wlV1fW8Yhu7X9+OOPp+OPPz69+eabafLkyWnq1KmddMVdV1vGYsqUKfk7oQ8cODCdcMIJ+fnZeHBo9h6HYrTX8/VhDZXWPo252HMOdjwdMw57Txy8/PLL818M2b9i6PyxyOYHZfMi7r///k64wu6hLeOQ/WvzjDPOSM8++2y69dZb01/+8pf0q1/9yhy6wzAWc+fOzSfRZndTNm7cmD755JN8/hadq72erw9rqPg05hjaMg5NbrvttrRly5Z05513duAVdh+ljkW2uiG7kzVz5sx8FRyH7+/E22+/nc4777zm29yDBw9OxxxzTH53hc4di2zSZhYmQ4YMycchm0g7f/78TrhaOuL5+rCGik9jjqEt45B55pln8uVmTzzxRPPrx3TuWGT/WnzllVfyX8rZvyCzLfta9jr+o48+2olX3rW05e9E9pLP3ktgs5cesoj3+6nzxyK70/vee+8172cr4Q70obl0rHZ7vi4cRh9//HHhmGOOKfz2t7/N96dMmVI4//zz8//eunVr4ZNPPtnvnGzG9+c+97nCv/71r0JjY2Nh5MiRhbvuuqvTr70racs4rF27Nh+Hhx56KB+HbGualU/njUV2fDb7fu9t0KBBhZdffjkfEzrv78Sf/vSnfMXV888/X9iwYUPhhz/8YeHLX/5y84oHOm8sslU/J554YmHBggWFX//614UvfvGLhYsvvrjTr72rSp9a9dPQ0FD46KOP9juuvr6+UFFRUfjzn/+cf//b3/52YerUqaU/XuEwe/rppwt9+vTJ/4Jn/2dcs2bN/19YSoWVK1ce8Jzrr7++0KtXr0JlZWVh1KhRhZ07d3byVXc9pY7DtGnT8u/tvQ0dOvQwXHnX05a/E3vLxsHy5MMzDvPmzSucdNJJ+S/nMWPGFNatW9fJV901lToWWcBkoZgdm43Fd77znfxJk44Jlex3TraM/EDuv//+whFHHJEvMa+pqSls3ry55McL8aGEPo05hraMAx3DWMRgHOIwFp9d2ct169atS2eddVab3j4hRKgAAIT8rB8AgJYIFQAgLKECAIQlVACAsIQKABCWUAEAwhIqAEBYQgUACEuoAAApqv8DOG3SuqPmAMIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1400x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(train_acts, train_preds, alpha=0.3)\n",
    "plt.title('训练集预测 vs 实际')\n",
    "plt.xlabel('实际值')\n",
    "plt.ylabel('预测值')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(val_acts, val_preds, alpha=0.3, color='orange')\n",
    "plt.title('验证集预测 vs 实际')\n",
    "plt.xlabel('实际值')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已保存为 crypto_predictor.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save({\n",
    "    'model_state': model.state_dict(),\n",
    "    'input_dim': windows[0].shape[-1],\n",
    "    'scaler': scaler\n",
    "}, 'crypto_predictor.pth')\n",
    "\n",
    "print(\"模型已保存为 crypto_predictor.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
